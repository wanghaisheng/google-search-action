name: Daily App Opportunity Hunter (Inputs & Dynamic Filenames V7)

on:
  workflow_dispatch:
    inputs:
      paywall_anger_query:
        description: 'Query for paywall anger detector'
        required: false
        default: '{category} app paywall scam subscription trap forced to pay rip off overpriced worthless'
      uninstall_intention_query:
        description: 'Query for uninstall intention detector'
        required: false
        default: '{category} app how to cancel subscription how to uninstall'
      worth_it_review_query:
        description: 'Query for worth it review detector'
        required: false
        default: '{category} app worth it review alternatives to best free'
      categories:
        description: 'Comma-separated list of categories (leave empty for broad search)'
        required: false
        default: ''
      timeframe:
        description: 'Timeframe for the search (qdr:h, qdr:d, qdr:w, qdr:m, qdr:y)'
        required: false
        default: 'qdr:d'

jobs:
  hunt-for-apps:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "googlesearch-python" pandas beautifulsoup4 requests

      - name: Run Google Search Scraper (CSV)
        id: scraper
        run: |
          cat << 'EOF' > csv_scraper.py
          import sys
          import requests
          from bs4 import BeautifulSoup
          from googlesearch import search
          import pandas as pd
          import time
          import os
          from datetime import datetime

          # Define a user-agent to mimic a real browser visit
          HEADERS = {
              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
          }

          def fetch_search_results(query, tbs="qdr:d1", num_results=100):
              """
              Fetches Google search results and scrapes the title and description for each URL.
              """
              results = []
              print(f"Fetching up to {num_results} results for query: '{query}'...")

              # Use the num_results to control the number of pages fetched
              for i, url in enumerate(search(query, tbs=tbs, num=num_results, stop=num_results, pause=2.0)):
                  if i >= num_results:
                      break
                  
                  print(f"[{i+1}/{num_results}] Processing: {url}")
                  
                  try:
                      # Fetch the content of the URL with a timeout
                      response = requests.get(url, headers=HEADERS, timeout=10)
                      response.raise_for_status()

                      soup = BeautifulSoup(response.content, 'html.parser')
                      
                      title_tag = soup.find('title')
                      title = title_tag.get_text().strip() if title_tag else "No Title Found"

                      description_tag = soup.find('meta', attrs={'name': 'description'})
                      description = description_tag['content'].strip() if description_tag and description_tag.get('content') else "No Description Found"

                      results.append({"URL": url, "Title": title, "Description": description})
                      
                  except requests.exceptions.RequestException as e:
                      print(f"  -> Error fetching {url}: {e}")
                      results.append({"URL": url, "Title": "Error - Could not fetch", "Description": str(e)})
                  except Exception as e:
                      print(f"  -> An unexpected error occurred for {url}: {e}")
                      results.append({"URL": url, "Title": "Error - Processing failed", "Description": str(e)})
                      
                  time.sleep(0.5)

              return results

          # --- UPDATED FUNCTION WITH APPEND & DEDUPLICATE LOGIC ---
          def save_to_csv(results, folder_name="results", filename="results.csv"):
              """
              Saves results to a CSV file with a specified filename in a specific folder.
              If the file already exists, it appends the new results and removes duplicates based on the URL.
              """
              if not results:
                  print("No results to save.")
                  return

              os.makedirs(folder_name, exist_ok=True)
              
              full_path = os.path.join(folder_name, filename)

              new_df = pd.DataFrame(results)

              if os.path.exists(full_path):
                  print(f"File '{full_path}' exists. Appending new data and de-duplicating...")
                  try:
                      # Read the existing data from the CSV
                      existing_df = pd.read_csv(full_path)
                      # Combine the old and new dataframes
                      combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                      # Drop duplicates based on the 'URL' column, keeping the last entry
                      final_df = combined_df.drop_duplicates(subset=['URL'], keep='last')
                      
                      # Calculate how many new rows were actually added
                      rows_added = len(final_df) - len(existing_df)
                      print(f"Appended {rows_added} new unique results.")

                  except pd.errors.EmptyDataError:
                      # If the existing file is empty, just use the new data
                      print("Existing file is empty. Saving new results.")
                      final_df = new_df
              else:
                  print(f"Creating new file: '{full_path}'")
                  final_df = new_df

              # Save the final, de-duplicated dataframe to the CSV
              final_df.to_csv(full_path, index=False, encoding='utf-8-sig')
              print(f"Successfully saved. Total unique results in file: {len(final_df)}")
          # --- END OF UPDATED FUNCTION ---


          if __name__ == "__main__":
              if len(sys.argv) < 5:
                  print("Usage: python your_script_name.py \"<query>\" <timeframe> <detector_name> <category>")
                  sys.exit(1)
                  
              query = sys.argv[1]
              timeframe = sys.argv[2]
              detector_name = sys.argv[3]
              category = sys.argv[4] # New: Capture the category
              tbs="qdr:d1" # Default to 24 hours
              
              if  timeframe:
                  tbs=timeframe
              
              results = fetch_search_results(query, tbs=tbs, num_results=100)

              # Use the detector name and category to generate a dynamic filename
              if category:
                 filename = f"{detector_name}_{category}.csv" # e.g., paywall_anger_photoeditor.csv
              else:
                 filename = f"{detector_name}_broad.csv"      # e.g., paywall_anger_broad.csv

              save_to_csv(results, folder_name="results", filename=filename)
          EOF

          # Define detectors here; the queries will be passed as inputs
          detectors='{ "paywall_anger": "${{ github.event.inputs.paywall_anger_query }}", "uninstall_intention": "${{ github.event.inputs.uninstall_intention_query }}", "worth_it_review": "${{ github.event.inputs.worth_it_review_query }}" }'

          # Categories are comma-separated in inputs
          categories="${{ github.event.inputs.categories }}"
          if [[ -z "$categories" ]]; then
             categories_array=("") # Empty array for broad search
          else
             IFS=',' read -r -a categories_array <<< "$categories"
          fi

          timeframe="${{ github.event.inputs.timeframe }}"

          # Loop through each detector and category to run the scraper
          for detector_name in $(echo "$detectors" | jq -r 'keys[]')
          do
            detector_query=$(echo "$detectors" | jq -r ".${detector_name}")

            for category in "${categories_array[@]}"
            do
               if [[ -z "$category" ]]; then
                  query="$detector_query"  # Use the raw query for broad search
               else
                  query=$(echo "$detector_query" | sed "s/\\{category\\}/$category/g") # Replace {category}
               fi
               #Execute the scraper with the dynamically created arguments
               python csv_scraper.py "$query" "$timeframe" "$detector_name" "$category"
            done
          done

      - name: Commit and Push CSV Results
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "CI: Daily App Opportunity Hunter (Inputs & Dynamic Filenames V7)"
          file_pattern: "results/*.csv"  # Track all CSV files in "results"
          commit_user_name: "GitHub Actions Bot"
          commit_user_email: "actions@github.com"
          commit_author: "GitHub Actions Bot <actions@github.com>"
